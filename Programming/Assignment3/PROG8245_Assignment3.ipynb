{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Exploring IR & NLP\n",
    "- In this assignment we are going to implement various IR techniques <b><i>From Scratch</i></b>, Please don't use available libraries except if specified that you can use it.\n",
    "- You are required to submit 6 different functions for this assignment, you can additional helper functions but only 6 will be tested.\n",
    "- You will be granted 10 marks for clean code and documenting the code.\n",
    "- Student Name: Tai Siang Huang\n",
    "- ID: 9006413\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language, python proved its importance in various domains.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in python code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART A: Preprocessing (15 Marks)\n",
    "- You are required to preprocess the text and apply the tokenization process.<br/>\n",
    "- Proprocessing should include tokenization, normalization, stemming <b>OR</b> lemmatization, and Named Entity Recognition (NER).<br/>\n",
    "- You need to make sure that Named Entities are not broken into separate tokens, but should be normalized by case-folding only. <br/>\n",
    "- The output of this step should be list of tokenized sentences. [[sentence1_token1, sentence1_token2, .. .], [sentence2_token1, .. .], .. .] <br/>\n",
    "- Please write the functionality of clean_sentences as explained in the comment (Please do comment your code at each essential step) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You are allowed for PART A to use any library that would help you in the task.\n",
    "def clean_sentences(sentences=None):\n",
    "    ## This function takes as an input list of sentences\n",
    "    tokenized_sentences = []\n",
    "    for i, sentence in enumerate(sentences, start=1):\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        # entities = doc.ents \n",
    "        # print(entities)  # only JavaScript and Java are recognized as entities, there are no inseparable NER \n",
    "\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # remove punctuation stop words and spaces\n",
    "            if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "                # normalize the token\n",
    "                token = token.text.lower()\n",
    "                # remove punctuation and digits\n",
    "                token = re.sub(r'[^a-zA-Z0-9 ]', '', token)\n",
    "\n",
    "                tokens.append(token)\n",
    "        # add the tokenized sentence to the list\n",
    "        tokenized_sentences.append(tokens)\n",
    "        # print(tokenized_sentences)\n",
    "\n",
    "    ## This function returns a list of tokenized_sentences\n",
    "    return tokenized_sentences\n",
    "\n",
    "# clean_sentences(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART B: Building IR Sentence-Word Representation (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>inverted index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the inverted index\n",
    "    output = {}\n",
    "    # start from 1 to match the sentence number\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "        # run through each token in the sentence\n",
    "        for token in sentence:\n",
    "            if token not in output:\n",
    "                #  create a new entry in the dictionary\n",
    "                output[token] = []\n",
    "            # append the sentence number to the list of the token\n",
    "            output[token].append(i)\n",
    "\n",
    "    return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE\n",
    "\n",
    "# get_inverted_index(clean_sentences(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>Positional index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "    output = {} \n",
    "    # start from 1 to match the sentence number\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "        # run through each token in the sentence\n",
    "        for position, token in enumerate(sentence):\n",
    "            # check if the token is already in the dictionary\n",
    "            if token not in output:\n",
    "                #  create a new entry in the dictionary\n",
    "                output[token] = {}\n",
    "\n",
    "            # check if the sentence number is already in the dictionary\n",
    "            if i not in output[token]:\n",
    "                #  initialize a new list for the sentence number\n",
    "                output[token][i] = []\n",
    "\n",
    "            # append the position of the token in the sentence\n",
    "            output[token][i].append(position)\n",
    "\n",
    "    return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE\n",
    "\n",
    "# get_positional_index(clean_sentences(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>TF-IDF Matrix</b> that is sufficient to represent the documents, the tokens are expected to be sorted as well as documentIDs. Assume that each sentence is a document and the sentence ID starts from 1. (10) You are not allowed to use any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the tf-idf matrix\n",
    "\n",
    "    # print(\"list_of_sentence_tokens\", list_of_sentence_tokens)\n",
    "    # unique tokens from the list\n",
    "    unique_tokens = set()\n",
    "    for sentence in list_of_sentence_tokens:\n",
    "        for token in sentence:\n",
    "            unique_tokens.add(token)\n",
    "    # unique_tokens = sorted(unique_tokens)\n",
    "\n",
    "    # compute the DF (Document Frequency) \n",
    "    DF = {}\n",
    "    for token in unique_tokens:\n",
    "        # initialize the DF of the token to 0\n",
    "        DF[token] = 0\n",
    "        # run through each sentence\n",
    "        for sentence in list_of_sentence_tokens:\n",
    "            if token in sentence:\n",
    "                # plus 1 to the DF of the token\n",
    "                DF[token] += 1\n",
    "    # print(\"DF\", DF)\n",
    "\n",
    "    # compute the IDF (Inverse Document Frequency) log(N/DF)\n",
    "    IDF = {}\n",
    "    # N is the number of sentences\n",
    "    N = len(list_of_sentence_tokens)\n",
    "    for token in unique_tokens:\n",
    "        IDF[token] = math.log(N / DF[token])\n",
    "    # print(\"IDF\", IDF)\n",
    "\n",
    "    \n",
    "    # convert to this structure [[token1_TF, token2_TF], [document2]]\n",
    "    # convert to this structure [[token1_TF_IDF, token2_TF_IDF], [document2]]\n",
    "    # create the same structure as the list_of_sentence_tokens but fill it with 0\n",
    "    # Initialize TF and TF_IDF matrices with zeros\n",
    "    TF = []\n",
    "    TF_IDF = []\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "        for j, token in enumerate(sentence):\n",
    "            # create a new list for each sentence\n",
    "            if i >= len(TF):\n",
    "                TF.append([0] * len(sentence))\n",
    "                TF_IDF.append([0] * len(sentence))\n",
    "            # initialize the TF and TF-IDF of the token to 0\n",
    "            TF[i][j] = 0\n",
    "            TF_IDF[i][j] = 0\n",
    "\n",
    "    # print(\"TF\", TF)\n",
    "    # print(\"TF_IDF\", TF_IDF)\n",
    "\n",
    "    # compute the TF (Term Frequency)\n",
    "    # (TF = count(token) / len(sentence))\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "        # print(\"sentence\",i, sentence)\n",
    "        # run through each token in the sentence\n",
    "        for j, token in enumerate(sentence):\n",
    "            # print(\"token\", j,token)\n",
    "            # count the number of times the token appears in the sentence\n",
    "            count = sentence.count(token)\n",
    "            # compute the TF of the token\n",
    "            TF[i][j] = count / len(sentence)\n",
    "    #         print(\"TF[i][j]\",i,j, TF[i][j])\n",
    "    #     print(\"TF[i]\",i, TF[i])\n",
    "    # print(\"TF\", TF)\n",
    "\n",
    "    # compute the TF-IDF matrix\n",
    "    # TF_IDF = TF * IDF\n",
    "    #  run through each sentence\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "        for j, token in enumerate(sentence):\n",
    "            # compute the TF-IDF of the token\n",
    "            TF_IDF[i][j] = TF[i][j] * IDF[token]\n",
    "\n",
    "    # print(\"TF_IDF\", TF_IDF)\n",
    "\n",
    "\n",
    "\n",
    "    return  TF_IDF #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE\n",
    "\n",
    "# get_TFIDF_matrix(clean_sentences(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART C- Measuring Documents Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a method that takes as an input: (15)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # TODO: Implement the functionality that returns the rank of the documents based on the method given and the search query\n",
    "    # Preprocess the query consistently with documents\n",
    "    query_tokens = clean_sentences([search_query])[0]\n",
    "    # unique tokens in the query\n",
    "    query_tokens_set = set(query_tokens)\n",
    "\n",
    "    # fill the scores with 0 [document_0_score, document_1_score, document_2_score, ...]\n",
    "    scores = [0] * len(list_of_sentence_tokens)\n",
    "    # print(\"scores\", scores)\n",
    "    ## If the method is \"inverted\" then rank the documents based on the number of matching tokens\n",
    "    if method_name == \"inverted\":\n",
    "        # get the inverted index\n",
    "        inverted_index = get_inverted_index(list_of_sentence_tokens)        \n",
    "        # run through each token in the sentence\n",
    "        for token in query_tokens_set:\n",
    "            # check if the token is in the inverted index\n",
    "            if token in inverted_index:\n",
    "                # run through each sentence that contains the token\n",
    "                # print(\"inverted_index[token]\",token, inverted_index[token])\n",
    "                for sentence_id in inverted_index[token]:\n",
    "                    # print(\"sentence_id\", sentence_id)\n",
    "                    # increment the score of the sentence\n",
    "                    scores[sentence_id - 1] += 1\n",
    "            else:\n",
    "                scores    \n",
    "        \n",
    "\n",
    "\n",
    "    ## If the method is \"tfidf\" then use the tfidf score equation in slides and return ranking based on the score\n",
    "    elif method_name == \"tfidf\":\n",
    "        # get the TF-IDF matrix\n",
    "        tfidf_matrix = get_TFIDF_matrix(list_of_sentence_tokens)\n",
    "\n",
    "        # get the index of the tokens in the TF-IDF matrix\n",
    "        # same token will be replaced with the TF-IDF score\n",
    "        # readable_conversion = {document1_id:{token1:TF_IDF, token2:TF_IDF}, document2_id:{token1:TF_IDF, token2:TF_IDF}}\n",
    "        readable_conversion = {}\n",
    "        for i, sentence_thidfs in enumerate(tfidf_matrix):\n",
    "            readable_conversion[i + 1] = {}\n",
    "            for j, tfidf in enumerate(sentence_thidfs):\n",
    "                # check if the tfidf is in the inverted index\n",
    "                if tfidf != 0:\n",
    "                    readable_conversion[i + 1][list_of_sentence_tokens[i][j]] = tfidf\n",
    "        # print(\"readable_conversion\", readable_conversion)\n",
    "\n",
    "        # run through each readable_conversion\n",
    "        for i, document in readable_conversion.items():\n",
    "            # print(\"document\", i, document)\n",
    "            # run through each token in the query\n",
    "            for token in query_tokens_set:\n",
    "                # check if the token is in the document\n",
    "                if token in document:\n",
    "                    # i - 1 because the document id starts from 1 \n",
    "                    scores[i - 1] += document[token]\n",
    "        # print(\"scores\", scores)\n",
    "    else:\n",
    "        raise ValueError(\"Method name must be either 'inverted' or 'tfidf'\")\n",
    "    ## The document with highest relevance should be ranked first\n",
    "    ## list method should return the index of the documents based on highest ranking first\n",
    "    rank_list = []\n",
    "    \n",
    "    # rank the sentences based on the score\n",
    "    rank_list = []\n",
    "    for i, score in enumerate(scores):\n",
    "        if score > 0: \n",
    "            rank_list.append((i, score))\n",
    "        else:\n",
    "            rank_list.append((i, 0))\n",
    "    \n",
    "    # sort the rank_list by score in descending order\n",
    "    rank_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    # get the document id\n",
    "    rank_list = [item[0] for item in rank_list]\n",
    "    return rank_list\n",
    "\n",
    "# get_ranked_documents(clean_sentences(sample_sentences), \"inverted\", \"Python programming\")\n",
    "# get_ranked_documents(clean_sentences(sample_sentences), \"tfidf\", \"Python programming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART D- TFIDF with a TWIST (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF with Custom Weighting Based on Document Length and Term Position\n",
    "- You are expected to implement a twisted version of the TF-IDF vectorizer, that incorporates two additional features:\n",
    "    - Document Length\n",
    "    - Term Position\n",
    "- This twist aims to assign weight based on Modified Term Frequency (MTF) and Modified inverse Document Frequency (MIDF)\n",
    "1. Modified Term Frequency (MTF):\n",
    "    - MTF is calculated by taking into consideration the position of the term into account\n",
    "    - The assumption is the closer the term appears to the beginning of the document, the higher the weight should be.\n",
    "    - $$\\text{MTF}(t, d) = \\frac{f(t, d)}{1 + \\text{position}(t, d)}$$\n",
    "        - Where f(t,d) is the raw count of term t in document d.\n",
    "        - position(t,d) is the position of the first occurence of term t in document d.\n",
    "2. Modified Inverse Document Frequency (MIDF):\n",
    "    - MIDF is calculated taking into consideration the document length.\n",
    "    - The assumption is that the IDF should be inversely proportion not only to the number of documents it appears at, but also to the average length of documents where the term appears. \n",
    "    - Hence, longer documents are less significant for a term's relevance.\n",
    "    - $$\\text{MIDF}(t) = \\log \\left( \\frac{N}{\\text{df}(t) \\times \\frac{1}{M} \\sum_{d \\in D_{t}} |d|} \\right)$$\n",
    "\n",
    "        - N is the total number of documents\n",
    "        - df(t) is the document frequency\n",
    "        - M is a constant for scaling\n",
    "        - $${\\sum_{d \\in D_{t}} |d|}$$\n",
    "                 is the sum of the lengths of all documents that contain t\n",
    "        - |d| is the length of document d\n",
    "3. Final Weight (MTF-MIDF):\n",
    "    - The Combined is calculated as : MTF(t,d)*MIDF(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-A: Implement the function logic for getting modified tf-idf weightings. (20 Marks)\n",
    "<b><u>NOTE: M is a scaling factor, setting it to 5 in our example would be sufficient. However, you need to explore what does increasing and decreasing it represent.</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modified_tfidf_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the modified tf-idf matrix\n",
    "    \n",
    "    # set a constant for scaling\n",
    "    M = 5\n",
    "    \n",
    "    # unique tokens from the list\n",
    "    unique_tokens = set()\n",
    "    for sentence in list_of_sentence_tokens:\n",
    "        for token in sentence:\n",
    "            unique_tokens.add(token)\n",
    "    # unique_tokens = sorted(unique_tokens)\n",
    "\n",
    "    # compute the DF (Document Frequency) and the sum of lengths of the sentences that contain the token\n",
    "    DF = {}\n",
    "    sum_lengths = {}\n",
    "    for token in unique_tokens:\n",
    "        # initialize the DF of the token to 0\n",
    "        DF[token] = 0\n",
    "        sum_lengths[token] = 0\n",
    "        # run through each sentence\n",
    "        for sentence in list_of_sentence_tokens:\n",
    "            if token in sentence:\n",
    "                # plus 1 to the DF of the token\n",
    "                DF[token] += 1\n",
    "                # compute the sum of lengths of the sentences that contain the token\n",
    "                sum_lengths[token] += len(sentence)\n",
    "\n",
    "    # print(\"DF\", DF)\n",
    "    # print(\"sum_lengths\", sum_lengths)\n",
    "\n",
    "    # compute the MIDF (Modified Inverse Document Frequency) log(N*M/(DF*sum_length_of_sentence))\n",
    "    MIDF = {}\n",
    "    # N is the number of sentences\n",
    "    N = len(list_of_sentence_tokens)\n",
    "    for token in unique_tokens:\n",
    "        MIDF[token] = math.log(N * M / (DF[token] * sum_lengths[token]))\n",
    "    # print(\"MIDF\", MIDF)\n",
    "\n",
    "    \n",
    "    # convert to this structure [[token1_MTF, token2_MTF], [document2]]\n",
    "    # convert to this structure [[token1_MTF_MIDF, token2_MTF_MIDF], [document2]]\n",
    "    # create the same structure as the list_of_sentence_tokens but fill it with 0\n",
    "    # Initialize MTF and MTF_MIDF matrices with zeros\n",
    "    MTF = []\n",
    "    MTF_MIDF = []\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "        for j, token in enumerate(sentence):\n",
    "            # create a new list for each sentence\n",
    "            if i >= len(MTF):\n",
    "                MTF.append([0] * len(sentence))\n",
    "                MTF_MIDF.append([0] * len(sentence))\n",
    "            # initialize the MTF and MTF-MIDF of the token to 0\n",
    "            MTF[i][j] = 0\n",
    "            MTF_MIDF[i][j] = 0\n",
    "\n",
    "    # print(\"MTF\", MTF)\n",
    "    # print(\"MTF_MIDF\", MTF_MIDF)\n",
    "\n",
    "    positional_index = get_positional_index(list_of_sentence_tokens)\n",
    "\n",
    "    # compute the MTF (Term Frequency)\n",
    "    # (MTF = count(token) / 1 + positions(token))\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "        # print(\"sentence\",i, sentence)\n",
    "        # run through each token in the sentence\n",
    "        for j, token in enumerate(sentence):\n",
    "\n",
    "            # print(\"token\", j,token)\n",
    "            if token in positional_index and i in positional_index[token]:\n",
    "                first_pos = positional_index[token][i][0]  # First position\n",
    "            else:\n",
    "                first_pos = 0\n",
    "            # print(\"first_pos\",first_pos)\n",
    "            # count the number of times the token appears in the sentence\n",
    "            count = sentence.count(token)\n",
    "            # compute the MTF of the token\n",
    "            MTF[i][j] = count / (1 + first_pos)\n",
    "        #     print(\"MTF[i][j]\",i,j, MTF[i][j])\n",
    "        # print(\"MTF[i]\",i, MTF[i])\n",
    "    # print(\"MTF\", MTF)\n",
    "\n",
    "    # compute the MTF-MIDF matrix\n",
    "    # MTF_MIDF = MTF * MIDF\n",
    "    #  run through each sentence\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "        for j, token in enumerate(sentence):\n",
    "            # compute the MTF-MIDF of the token\n",
    "            MTF_MIDF[i][j] = MTF[i][j] * MIDF[token]\n",
    "\n",
    "    # print(\"MTF_MIDF\", MTF_MIDF)\n",
    "\n",
    "\n",
    "    return  MTF_MIDF #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE\n",
    "\n",
    "# get_modified_tfidf_matrix(clean_sentences(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-B: Experiment the effect of changing M and comment on what do you think M is for and why is it added. (5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **M** is a scaling factor in the MIDF formula that balances the denominator, which the both number of documents a term appears in and their total length.\n",
    "- If term appears in `longer` documents, the average length of the documents will `increase`, causing the denominator to `increase`, thereby `reducing` the MIDF value.\n",
    "- If term appears in `shorter` documents, the average length of the documents will `decrease`, causing the denominator to `decrease`, thereby `increasing` the MIDF value.\n",
    "- Additionally,\n",
    "    - Increasing **M** will reduce the penalty for terms appearing in longer documents.\n",
    "    - Decreasing **M** will increase the penalty for terms in longer documents.\n",
    "- The reason of add **M**, can represent how document length impacts term relevance then providing flexibility for different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-C: Do you think Modified TF-Modified IDF is a good technique? Please comment and explain your thoughts.(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <u>**MTF is sensitivity with the position**</u>, prioritizes terms appearing earlier, which for the documents where the beginning, like titles, is more significant.\n",
    "- <u>**MIDF penalizes terms in widely documents**</u>, could potentially improve the connection between collections with varying document sizes.\n",
    "- <u>**Effectiveness depends on the context**</u>, it is a good technique for applications where term position and document length are critical, for instance news articles.\n",
    "- However, it should be used carefully in diverse research corpora where document length and structure vary widely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
