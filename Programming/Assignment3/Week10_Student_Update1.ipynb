{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oavg7XmffPfB"
      },
      "source": [
        "# Week 10 Lab:\n",
        "- Bag of Words\n",
        "- tf-idf\n",
        "- Euclidean Distance vs Cosine Similarity\n",
        "- Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whxz_c9nfPfC"
      },
      "source": [
        "Gensim: Gensim is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) and painlessly (human-wise) as possible. <br/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1D_qBQtfPfD",
        "outputId": "c221640c-bdb8-4bd9-b20f-3a43b70f84c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPGWuV-HfPfD"
      },
      "source": [
        "Sample text retrieved from Wikipedia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57CEyst7fPfD"
      },
      "outputs": [],
      "source": [
        "txt = '''Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication,\n",
        "personal development and psychotherapy, that first appeared in Richard Bandler and John Grinder's 1975 book The Structure of Magic I.\n",
        " NLP asserts that there is a connection between neurological processes (neuro-), language (linguistic) and acquired behavioral patterns (programming),\n",
        "and that these can be changed to achieve specific goals in life. According to Bandler and Grinder, NLP can treat problems such as phobias, depression,\n",
        "tic disorders, psychosomatic illnesses, near-sightedness, allergy, the common cold, and learning disorders, often in a single session.\n",
        "They also claim that NLP can model the skills of exceptional people, allowing anyone to acquire them.\n",
        "NLP has been adopted by some hypnotherapists, as well as by companies that run seminars marketed as leadership training to businesses and government agencies.\n",
        "There is no scientific evidence supporting the claims made by NLP advocates, and it has been called a pseudoscience.\n",
        "Scientific reviews have shown that NLP is based on outdated metaphors of the brain's inner workings that are inconsistent with current neurological theory,\n",
        "and contain numerous factual errors. Reviews also found that research that favored NLP contained significant methodological flaws,\n",
        "and that there were three times as many studies of a much higher quality that failed to reproduce the extraordinary claims made by Bandler,\n",
        "Grinder, and other NLP practitioners.'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnU9XEApfPfD"
      },
      "outputs": [],
      "source": [
        "# importing the needed libraries\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim import corpora\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc6OVoIOfPfE"
      },
      "source": [
        "Tokenization (Revisted): Lets use sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt'), nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjqA8UuNfewf",
        "outputId": "280bf1d7-53df-43e2-e9d5-846fbe6db527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUygvXmufPfE",
        "outputId": "174fd93a-888b-4bea-eb31-804dded14538"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication, \\npersonal development and psychotherapy, that first appeared in Richard Bandler and John Grinder's 1975 book The Structure of Magic I. \\n NLP asserts that there is a connection between neurological processes (neuro-), language (linguistic) and acquired behavioral patterns (programming), \\nand that these can be changed to achieve specific goals in life.\",\n",
              " 'According to Bandler and Grinder, NLP can treat problems such as phobias, depression, \\ntic disorders, psychosomatic illnesses, near-sightedness, allergy, the common cold, and learning disorders, often in a single session.',\n",
              " 'They also claim that NLP can model the skills of exceptional people, allowing anyone to acquire them.',\n",
              " 'NLP has been adopted by some hypnotherapists, as well as by companies that run seminars marketed as leadership training to businesses and government agencies.',\n",
              " 'There is no scientific evidence supporting the claims made by NLP advocates, and it has been called a pseudoscience.',\n",
              " \"Scientific reviews have shown that NLP is based on outdated metaphors of the brain's inner workings that are inconsistent with current neurological theory, \\nand contain numerous factual errors.\",\n",
              " 'Reviews also found that research that favored NLP contained significant methodological flaws, \\nand that there were three times as many studies of a much higher quality that failed to reproduce the extraordinary claims made by Bandler, \\nGrinder, and other NLP practitioners.']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "sentences = nltk.sent_tokenize(txt)\n",
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVjSkDgpfPfE"
      },
      "source": [
        "Cleaning: <br>\n",
        "1. Remove extra spaces\n",
        "2. Convert sentences to lower case\n",
        "3. Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "kDcyMH3QfPfE",
        "outputId": "bcfda7d1-c163-4525-d479-36560322b1f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication, \\npersonal development and psychotherapy, that first appeared in Richard Bandler and John Grinder's 1975 book The Structure of Magic I. \\n NLP asserts that there is a connection between neurological processes (neuro-), language (linguistic) and acquired behavioral patterns (programming), \\nand that these can be changed to achieve specific goals in life.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF5X0DIffPfE"
      },
      "outputs": [],
      "source": [
        "def clean_sentence(sentence):\n",
        "    # Input: One sentence\n",
        "    # Clean the sentence and\n",
        "    # Return cleaned version of the sentence\n",
        "\n",
        "    # Replacing '-' with ' '\n",
        "    sentence_cleaned_v1 = re.sub(r'-',' ', sentence)\n",
        "    regular_expression_num_letters = r'[^a-zA-Z0-9 ]'\n",
        "    # Removing any character which is not a space, letter or a number\n",
        "    sentence_cleaned_v2 = re.sub(regular_expression_num_letters,'',sentence_cleaned_v1)\n",
        "    # Remove stopwords using Gensim's function\n",
        "    output = remove_stopwords(sentence_cleaned_v2.lower())\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "D2csAEB8fPfE",
        "outputId": "9444437b-ddcf-4c8f-f1d1-5e47a35fe43e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neuro linguistic programming nlp pseudoscientific approach communication personal development psychotherapy appeared richard bandler john grinders 1975 book structure magic nlp asserts connection neurological processes neuro language linguistic acquired behavioral patterns programming changed achieve specific goals life'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "clean_sentence(sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ar_jRbefPfF",
        "outputId": "86ee74bb-3e86-4beb-fd83-b6c5c5fe049f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "337 179\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([\"you're\",\n",
              "  \"you've\",\n",
              "  \"you'll\",\n",
              "  \"you'd\",\n",
              "  \"she's\",\n",
              "  \"it's\",\n",
              "  'theirs',\n",
              "  \"that'll\",\n",
              "  'having',\n",
              "  's',\n",
              "  't',\n",
              "  \"don't\",\n",
              "  \"should've\",\n",
              "  'd',\n",
              "  'll',\n",
              "  'm',\n",
              "  'o',\n",
              "  've',\n",
              "  'y',\n",
              "  'ain',\n",
              "  'aren',\n",
              "  \"aren't\",\n",
              "  'couldn',\n",
              "  \"couldn't\",\n",
              "  \"didn't\",\n",
              "  \"doesn't\",\n",
              "  'hadn',\n",
              "  \"hadn't\",\n",
              "  'hasn',\n",
              "  \"hasn't\",\n",
              "  'haven',\n",
              "  \"haven't\",\n",
              "  'isn',\n",
              "  \"isn't\",\n",
              "  'ma',\n",
              "  'mightn',\n",
              "  \"mightn't\",\n",
              "  'mustn',\n",
              "  \"mustn't\",\n",
              "  'needn',\n",
              "  \"needn't\",\n",
              "  'shan',\n",
              "  \"shan't\",\n",
              "  'shouldn',\n",
              "  \"shouldn't\",\n",
              "  'wasn',\n",
              "  \"wasn't\",\n",
              "  'weren',\n",
              "  \"weren't\",\n",
              "  'won',\n",
              "  \"won't\",\n",
              "  'wouldn',\n",
              "  \"wouldn't\"],\n",
              " ['amoungst',\n",
              "  'de',\n",
              "  'nine',\n",
              "  'thence',\n",
              "  'various',\n",
              "  'bill',\n",
              "  'indeed',\n",
              "  'others',\n",
              "  'thick',\n",
              "  'become',\n",
              "  'computer',\n",
              "  'whole',\n",
              "  'ltd',\n",
              "  'twenty',\n",
              "  'throughout',\n",
              "  'inc',\n",
              "  'onto',\n",
              "  'latter',\n",
              "  'eight',\n",
              "  'one',\n",
              "  'full',\n",
              "  'per',\n",
              "  'km',\n",
              "  'anyway',\n",
              "  'many',\n",
              "  'whence',\n",
              "  'due',\n",
              "  'serious',\n",
              "  'thereby',\n",
              "  'whereas',\n",
              "  'eleven',\n",
              "  'almost',\n",
              "  'namely',\n",
              "  'always',\n",
              "  'without',\n",
              "  'fill',\n",
              "  'nowhere',\n",
              "  'wherein',\n",
              "  'co',\n",
              "  'mostly',\n",
              "  'never',\n",
              "  'something',\n",
              "  'often',\n",
              "  'perhaps',\n",
              "  'must',\n",
              "  'less',\n",
              "  'name',\n",
              "  'together',\n",
              "  'except',\n",
              "  'hasnt',\n",
              "  'noone',\n",
              "  'call',\n",
              "  'hundred',\n",
              "  'give',\n",
              "  'beside',\n",
              "  'us',\n",
              "  'using',\n",
              "  'hereafter',\n",
              "  'therefore',\n",
              "  'couldnt',\n",
              "  'else',\n",
              "  'across',\n",
              "  'along',\n",
              "  'sometimes',\n",
              "  'whoever',\n",
              "  'alone',\n",
              "  'thin',\n",
              "  'hence',\n",
              "  'cannot',\n",
              "  'therein',\n",
              "  'besides',\n",
              "  'thereafter',\n",
              "  'still',\n",
              "  'made',\n",
              "  'three',\n",
              "  'see',\n",
              "  'system',\n",
              "  'fire',\n",
              "  'ever',\n",
              "  'yet',\n",
              "  'however',\n",
              "  'thus',\n",
              "  'back',\n",
              "  'somehow',\n",
              "  'side',\n",
              "  'two',\n",
              "  'part',\n",
              "  'via',\n",
              "  'formerly',\n",
              "  'moreover',\n",
              "  'anything',\n",
              "  'anyhow',\n",
              "  'elsewhere',\n",
              "  'also',\n",
              "  'take',\n",
              "  'another',\n",
              "  'every',\n",
              "  'done',\n",
              "  'upon',\n",
              "  'would',\n",
              "  'could',\n",
              "  'used',\n",
              "  'interest',\n",
              "  'sincere',\n",
              "  'becomes',\n",
              "  'none',\n",
              "  'etc',\n",
              "  'whenever',\n",
              "  'ie',\n",
              "  'among',\n",
              "  'otherwise',\n",
              "  'beyond',\n",
              "  'seem',\n",
              "  'thru',\n",
              "  'amongst',\n",
              "  'third',\n",
              "  'beforehand',\n",
              "  'seeming',\n",
              "  'say',\n",
              "  'first',\n",
              "  'everywhere',\n",
              "  'make',\n",
              "  'nevertheless',\n",
              "  'hereupon',\n",
              "  'herein',\n",
              "  'mine',\n",
              "  'forty',\n",
              "  'thereupon',\n",
              "  'empty',\n",
              "  'describe',\n",
              "  'detail',\n",
              "  'please',\n",
              "  'con',\n",
              "  'whatever',\n",
              "  'latterly',\n",
              "  'move',\n",
              "  'wherever',\n",
              "  'find',\n",
              "  'whereby',\n",
              "  'fifteen',\n",
              "  'nobody',\n",
              "  'rather',\n",
              "  'neither',\n",
              "  'cant',\n",
              "  'hereby',\n",
              "  'former',\n",
              "  'behind',\n",
              "  'five',\n",
              "  'fifty',\n",
              "  'around',\n",
              "  'either',\n",
              "  'several',\n",
              "  'quite',\n",
              "  'get',\n",
              "  'whether',\n",
              "  'twelve',\n",
              "  'anywhere',\n",
              "  'somewhere',\n",
              "  'becoming',\n",
              "  'toward',\n",
              "  'someone',\n",
              "  'seemed',\n",
              "  'go',\n",
              "  'amount',\n",
              "  'already',\n",
              "  'though',\n",
              "  'show',\n",
              "  'became',\n",
              "  'un',\n",
              "  'sometime',\n",
              "  'everyone',\n",
              "  'might',\n",
              "  'ten',\n",
              "  'bottom',\n",
              "  'towards',\n",
              "  'afterwards',\n",
              "  'much',\n",
              "  'whither',\n",
              "  'nothing',\n",
              "  'six',\n",
              "  'anyone',\n",
              "  'although',\n",
              "  'well',\n",
              "  'regarding',\n",
              "  'least',\n",
              "  'last',\n",
              "  'enough',\n",
              "  'meanwhile',\n",
              "  'found',\n",
              "  'kg',\n",
              "  'unless',\n",
              "  'even',\n",
              "  'put',\n",
              "  'four',\n",
              "  'cry',\n",
              "  'sixty',\n",
              "  'seems',\n",
              "  'keep',\n",
              "  'eg',\n",
              "  'everything',\n",
              "  'within',\n",
              "  'whose',\n",
              "  'whereafter',\n",
              "  'mill',\n",
              "  'may',\n",
              "  'since',\n",
              "  'whereupon',\n",
              "  'next',\n",
              "  'top',\n",
              "  'really',\n",
              "  'front'])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "gensim_stopwords = list(gensim.parsing.preprocessing.STOPWORDS)\n",
        "nltk_stopwords = stopwords.words('english')\n",
        "print(len(gensim_stopwords), len(nltk_stopwords))\n",
        "word_difference = [word for word in gensim_stopwords if word not in nltk_stopwords]\n",
        "word_difference_nltk = [word for word in nltk_stopwords if word not in gensim_stopwords]\n",
        "word_difference_nltk, word_difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_RNbv9FfPfF"
      },
      "source": [
        "Getting as input array of sentences and returning cleaned sentences array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btzuO5oUfPfF"
      },
      "outputs": [],
      "source": [
        "def get_cleaned_sentences(sentences):\n",
        "\n",
        "    return [clean_sentence(sentence) for sentence in sentences]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAFpqOHdfPfF",
        "outputId": "9d7aac3d-46bc-43e5-9bee-37f6748a9ab3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neuro linguistic programming nlp pseudoscientific approach communication personal development psychotherapy appeared richard bandler john grinders 1975 book structure magic nlp asserts connection neurological processes neuro language linguistic acquired behavioral patterns programming changed achieve specific goals life',\n",
              " 'according bandler grinder nlp treat problems phobias depression tic disorders psychosomatic illnesses near sightedness allergy common cold learning disorders single session',\n",
              " 'claim nlp model skills exceptional people allowing acquire',\n",
              " 'nlp adopted hypnotherapists companies run seminars marketed leadership training businesses government agencies',\n",
              " 'scientific evidence supporting claims nlp advocates called pseudoscience',\n",
              " 'scientific reviews shown nlp based outdated metaphors brains inner workings inconsistent current neurological theory contain numerous factual errors',\n",
              " 'reviews research favored nlp contained significant methodological flaws times studies higher quality failed reproduce extraordinary claims bandler grinder nlp practitioners']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "cleaned_sent = get_cleaned_sentences(sentences)\n",
        "cleaned_sent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrdqwFxfPfF"
      },
      "source": [
        "### Bag of Word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMVJ2Av5fPfF"
      },
      "source": [
        "Where all the unique words in the dictionary are organized as: <br>\n",
        "key: unique word and value: count/frequency<br>\n",
        "Then we are using function doc2bow which is used to create word embedding and storing all the word embedding to the corpus.<br>\n",
        "Dictionary is created organized in Asc order. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-6-p8ufPfG",
        "outputId": "86bc80d9-28ac-407b-973c-14fd215cb648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123 104\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1975': 0,\n",
              " 'according': 1,\n",
              " 'achieve': 2,\n",
              " 'acquire': 3,\n",
              " 'acquired': 4,\n",
              " 'adopted': 5,\n",
              " 'advocates': 6,\n",
              " 'agencies': 7,\n",
              " 'allergy': 8,\n",
              " 'allowing': 9,\n",
              " 'appeared': 10,\n",
              " 'approach': 11,\n",
              " 'asserts': 12,\n",
              " 'bandler': 13,\n",
              " 'based': 14,\n",
              " 'behavioral': 15,\n",
              " 'book': 16,\n",
              " 'brains': 17,\n",
              " 'businesses': 18,\n",
              " 'called': 19,\n",
              " 'changed': 20,\n",
              " 'claim': 21,\n",
              " 'claims': 22,\n",
              " 'cold': 23,\n",
              " 'common': 24,\n",
              " 'communication': 25,\n",
              " 'companies': 26,\n",
              " 'connection': 27,\n",
              " 'contain': 28,\n",
              " 'contained': 29,\n",
              " 'current': 30,\n",
              " 'depression': 31,\n",
              " 'development': 32,\n",
              " 'disorders': 33,\n",
              " 'errors': 34,\n",
              " 'evidence': 35,\n",
              " 'exceptional': 36,\n",
              " 'extraordinary': 37,\n",
              " 'factual': 38,\n",
              " 'failed': 39,\n",
              " 'favored': 40,\n",
              " 'flaws': 41,\n",
              " 'goals': 42,\n",
              " 'government': 43,\n",
              " 'grinder': 44,\n",
              " 'grinders': 45,\n",
              " 'higher': 46,\n",
              " 'hypnotherapists': 47,\n",
              " 'illnesses': 48,\n",
              " 'inconsistent': 49,\n",
              " 'inner': 50,\n",
              " 'john': 51,\n",
              " 'language': 52,\n",
              " 'leadership': 53,\n",
              " 'learning': 54,\n",
              " 'life': 55,\n",
              " 'linguistic': 56,\n",
              " 'magic': 57,\n",
              " 'marketed': 58,\n",
              " 'metaphors': 59,\n",
              " 'methodological': 60,\n",
              " 'model': 61,\n",
              " 'near': 62,\n",
              " 'neuro': 63,\n",
              " 'neurological': 64,\n",
              " 'nlp': 65,\n",
              " 'numerous': 66,\n",
              " 'outdated': 67,\n",
              " 'patterns': 68,\n",
              " 'people': 69,\n",
              " 'personal': 70,\n",
              " 'phobias': 71,\n",
              " 'practitioners': 72,\n",
              " 'problems': 73,\n",
              " 'processes': 74,\n",
              " 'programming': 75,\n",
              " 'pseudoscience': 76,\n",
              " 'pseudoscientific': 77,\n",
              " 'psychosomatic': 78,\n",
              " 'psychotherapy': 79,\n",
              " 'quality': 80,\n",
              " 'reproduce': 81,\n",
              " 'research': 82,\n",
              " 'reviews': 83,\n",
              " 'richard': 84,\n",
              " 'run': 85,\n",
              " 'scientific': 86,\n",
              " 'seminars': 87,\n",
              " 'session': 88,\n",
              " 'shown': 89,\n",
              " 'sightedness': 90,\n",
              " 'significant': 91,\n",
              " 'single': 92,\n",
              " 'skills': 93,\n",
              " 'specific': 94,\n",
              " 'structure': 95,\n",
              " 'studies': 96,\n",
              " 'supporting': 97,\n",
              " 'theory': 98,\n",
              " 'tic': 99,\n",
              " 'times': 100,\n",
              " 'training': 101,\n",
              " 'treat': 102,\n",
              " 'workings': 103}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# dictionary= {word1:id, word2:id, ...}\n",
        "# Confirm that words are sorted alphabetically\n",
        "# first: extract the words in the documents\n",
        "sentence_words = [sentence.split() for sentence in cleaned_sent]\n",
        "# Get the sorted words/tokens\n",
        "tokens = []\n",
        "for words in sentence_words:\n",
        "    tokens.extend(words)\n",
        "tokens_unique = list(set(tokens))\n",
        "print(len(tokens), len(tokens_unique))\n",
        "tokens_unique.sort()\n",
        "tokens_dictionary = {}\n",
        "\n",
        "for i in range(len(tokens_unique)):\n",
        "    tokens_dictionary[tokens_unique[i]]=i\n",
        "tokens_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiIRJEWmfPfG"
      },
      "source": [
        "Checking content of dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsXDkQjlfPfG",
        "outputId": "5bcad7d3-5f73-497c-9b07-51bcd757612d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1975': 0,\n",
              " 'achieve': 1,\n",
              " 'acquired': 2,\n",
              " 'appeared': 3,\n",
              " 'approach': 4,\n",
              " 'asserts': 5,\n",
              " 'bandler': 6,\n",
              " 'behavioral': 7,\n",
              " 'book': 8,\n",
              " 'changed': 9,\n",
              " 'communication': 10,\n",
              " 'connection': 11,\n",
              " 'development': 12,\n",
              " 'goals': 13,\n",
              " 'grinders': 14,\n",
              " 'john': 15,\n",
              " 'language': 16,\n",
              " 'life': 17,\n",
              " 'linguistic': 18,\n",
              " 'magic': 19,\n",
              " 'neuro': 20,\n",
              " 'neurological': 21,\n",
              " 'nlp': 22,\n",
              " 'patterns': 23,\n",
              " 'personal': 24,\n",
              " 'processes': 25,\n",
              " 'programming': 26,\n",
              " 'pseudoscientific': 27,\n",
              " 'psychotherapy': 28,\n",
              " 'richard': 29,\n",
              " 'specific': 30,\n",
              " 'structure': 31,\n",
              " 'according': 32,\n",
              " 'allergy': 33,\n",
              " 'cold': 34,\n",
              " 'common': 35,\n",
              " 'depression': 36,\n",
              " 'disorders': 37,\n",
              " 'grinder': 38,\n",
              " 'illnesses': 39,\n",
              " 'learning': 40,\n",
              " 'near': 41,\n",
              " 'phobias': 42,\n",
              " 'problems': 43,\n",
              " 'psychosomatic': 44,\n",
              " 'session': 45,\n",
              " 'sightedness': 46,\n",
              " 'single': 47,\n",
              " 'tic': 48,\n",
              " 'treat': 49,\n",
              " 'acquire': 50,\n",
              " 'allowing': 51,\n",
              " 'claim': 52,\n",
              " 'exceptional': 53,\n",
              " 'model': 54,\n",
              " 'people': 55,\n",
              " 'skills': 56,\n",
              " 'adopted': 57,\n",
              " 'agencies': 58,\n",
              " 'businesses': 59,\n",
              " 'companies': 60,\n",
              " 'government': 61,\n",
              " 'hypnotherapists': 62,\n",
              " 'leadership': 63,\n",
              " 'marketed': 64,\n",
              " 'run': 65,\n",
              " 'seminars': 66,\n",
              " 'training': 67,\n",
              " 'advocates': 68,\n",
              " 'called': 69,\n",
              " 'claims': 70,\n",
              " 'evidence': 71,\n",
              " 'pseudoscience': 72,\n",
              " 'scientific': 73,\n",
              " 'supporting': 74,\n",
              " 'based': 75,\n",
              " 'brains': 76,\n",
              " 'contain': 77,\n",
              " 'current': 78,\n",
              " 'errors': 79,\n",
              " 'factual': 80,\n",
              " 'inconsistent': 81,\n",
              " 'inner': 82,\n",
              " 'metaphors': 83,\n",
              " 'numerous': 84,\n",
              " 'outdated': 85,\n",
              " 'reviews': 86,\n",
              " 'shown': 87,\n",
              " 'theory': 88,\n",
              " 'workings': 89,\n",
              " 'contained': 90,\n",
              " 'extraordinary': 91,\n",
              " 'failed': 92,\n",
              " 'favored': 93,\n",
              " 'flaws': 94,\n",
              " 'higher': 95,\n",
              " 'methodological': 96,\n",
              " 'practitioners': 97,\n",
              " 'quality': 98,\n",
              " 'reproduce': 99,\n",
              " 'research': 100,\n",
              " 'significant': 101,\n",
              " 'studies': 102,\n",
              " 'times': 103}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# Using Gensim\n",
        "dictionary_gensim = corpora.Dictionary(sentence_words)\n",
        "dictionary_gensim.token2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDYrxB14fPfG"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDySzuuffPfG"
      },
      "source": [
        "To get the TF-IDF vector we are going to use TfidfVectorizer from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O32v-oj-fPfG",
        "outputId": "5754c559-3b56-4b3b-f8c7-3b1b13847d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 104 117\n"
          ]
        }
      ],
      "source": [
        "print(len(cleaned_sent), len(tokens_unique), sum([len(list(set(sentence_word))) for sentence_word in sentence_words]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ta9yBnmfPfG",
        "outputId": "4106bb91-c50a-49c4-8651-6927c557486b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7x104 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 117 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "# fits on list of documents (list of paragraphs/sentence which more preferable be cleaned list)\n",
        "vectorizer.fit(cleaned_sent)\n",
        "# transforms this list into a matrix where each entry is a TFIDF Score (which is a score computed) to show the relevance\n",
        "# of each word in each document\n",
        "tfidf_vectors = vectorizer.transform(cleaned_sent)\n",
        "tfidf_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcUowP8OfPfH",
        "outputId": "c3bce48d-b6ef-431d-eec7-0ad9a7a1647d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1975', 'according', 'achieve', 'acquire', 'acquired', 'adopted',\n",
              "       'advocates', 'agencies', 'allergy', 'allowing', 'appeared',\n",
              "       'approach', 'asserts', 'bandler', 'based', 'behavioral', 'book',\n",
              "       'brains', 'businesses', 'called', 'changed', 'claim', 'claims',\n",
              "       'cold', 'common', 'communication', 'companies', 'connection',\n",
              "       'contain', 'contained', 'current', 'depression', 'development',\n",
              "       'disorders', 'errors', 'evidence', 'exceptional', 'extraordinary',\n",
              "       'factual', 'failed', 'favored', 'flaws', 'goals', 'government',\n",
              "       'grinder', 'grinders', 'higher', 'hypnotherapists', 'illnesses',\n",
              "       'inconsistent', 'inner', 'john', 'language', 'leadership',\n",
              "       'learning', 'life', 'linguistic', 'magic', 'marketed', 'metaphors',\n",
              "       'methodological', 'model', 'near', 'neuro', 'neurological', 'nlp',\n",
              "       'numerous', 'outdated', 'patterns', 'people', 'personal',\n",
              "       'phobias', 'practitioners', 'problems', 'processes', 'programming',\n",
              "       'pseudoscience', 'pseudoscientific', 'psychosomatic',\n",
              "       'psychotherapy', 'quality', 'reproduce', 'research', 'reviews',\n",
              "       'richard', 'run', 'scientific', 'seminars', 'session', 'shown',\n",
              "       'sightedness', 'significant', 'single', 'skills', 'specific',\n",
              "       'structure', 'studies', 'supporting', 'theory', 'tic', 'times',\n",
              "       'training', 'treat', 'workings'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOhOaUr1fPfH",
        "outputId": "8a96f515-1444-40c1-8c56-8a31ebd55785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0 TFIDF Vector: [[0.15832198 0.         0.15832198 0.         0.15832198 0.\n",
            "  0.         0.         0.         0.         0.15832198 0.15832198\n",
            "  0.15832198 0.11233417 0.         0.15832198 0.15832198 0.\n",
            "  0.         0.         0.15832198 0.         0.         0.\n",
            "  0.         0.15832198 0.         0.15832198 0.         0.\n",
            "  0.         0.         0.15832198 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.15832198 0.         0.         0.15832198 0.         0.\n",
            "  0.         0.         0.         0.15832198 0.15832198 0.\n",
            "  0.         0.15832198 0.31664395 0.15832198 0.         0.\n",
            "  0.         0.         0.         0.31664395 0.13142084 0.13269275\n",
            "  0.         0.         0.15832198 0.         0.15832198 0.\n",
            "  0.         0.         0.15832198 0.31664395 0.         0.15832198\n",
            "  0.         0.15832198 0.         0.         0.         0.\n",
            "  0.15832198 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.15832198 0.15832198\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Document 0 TFIDF Vector: {tfidf_vectors[0].toarray()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNZAnr2afPfH"
      },
      "source": [
        "To find the distance between two vectors we can either:\n",
        "- Calculate Euclidean Distance (The higher, less similar. not sufficient)\n",
        "- Calculate Cosine Similarity (The higher, more similar, sufficient)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  'scientific evidence supporting claims nlp advocates called pseudoscience',\n",
        "#  'scientific reviews shown nlp based outdated metaphors brains inner workings inconsistent current neurological theory contain numerous factual errors'"
      ],
      "metadata": {
        "id": "xfzBh6F_mp6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "convqwUXfPfH",
        "outputId": "990c11a4-3322-4ef8-fd51-31427cde5e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 104) (1, 104)\n"
          ]
        }
      ],
      "source": [
        "vector1 = tfidf_vectors[4].copy().toarray()\n",
        "vector2 = tfidf_vectors[5].copy().toarray()\n",
        "print(vector1.shape, vector2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkaRGroBfPfH",
        "outputId": "5616450a-4ebf-4cb0-8ccf-f0c15a75c4ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.353658081965788, array([[1.35365808]]))"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Lets calculate Euclidean Distance from vector1 to vector2\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "distance_vec1_vec2 = euclidean_distances(vector1, vector2)\n",
        "vector_diff = vector1-vector2\n",
        "np.linalg.norm(vector_diff), distance_vec1_vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4hjMLXUfPfH",
        "outputId": "412149f9-532b-4ee9-e6ba-78bd61d75e02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.0838049]]), array([[0.0838049]]))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# Cosine Similarity Between 2 Vectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_vec1_vec2 = cosine_similarity(vector1, vector2)\n",
        "dot_product = np.dot(vector1, vector2.T) # Vector1 DOT Vector2 (T)\n",
        "magnitude = (np.linalg.norm(vector1)*np.linalg.norm(vector2)) ## ||vector1|| * ||vector2||\n",
        "dot_product/magnitude , similarity_vec1_vec2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "befE4ceRfPfH"
      },
      "source": [
        "## Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIAoND5YfPfH",
        "outputId": "6c64f402-5616-43c2-a6ec-244c7ae3ba4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCM-qssafPfI"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(text)\n",
        "print(doc)\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHbSqHYufqbl",
        "outputId": "d4ba4db4-ace5-4eac-a6f2-b2463798b22d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the lazy dog.\n",
            "The the DET True\n",
            "quick quick ADJ False\n",
            "brown brown ADJ False\n",
            "fox fox NOUN False\n",
            "jumps jump VERB False\n",
            "over over ADP True\n",
            "the the DET True\n",
            "lazy lazy ADJ False\n",
            "dog dog NOUN False\n",
            ". . PUNCT False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkgbdmDUfwmg",
        "outputId": "ad2c63ff-c73b-4615-a05c-62689aa827ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    text  lemma    POS      explain  stopword\n",
            "0    The    the    DET   determiner      True\n",
            "1  quick  quick    ADJ    adjective     False\n",
            "2  brown  brown    ADJ    adjective     False\n",
            "3    fox    fox   NOUN         noun     False\n",
            "4  jumps   jump   VERB         verb     False\n",
            "5   over   over    ADP   adposition      True\n",
            "6    the    the    DET   determiner      True\n",
            "7   lazy   lazy    ADJ    adjective     False\n",
            "8    dog    dog   NOUN         noun     False\n",
            "9      .      .  PUNCT  punctuation     False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"dep\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "-nD-a0sxfzjc",
        "outputId": "7535df00-b055-40e3-e286-ba243dd2c99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cd0e62fb6a574a9ab4493c026906d16c-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">quick</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">brown</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">fox</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">jumps</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">over</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">lazy</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">dog.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cd0e62fb6a574a9ab4493c026906d16c-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cd0e62fb6a574a9ab4493c026906d16c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cd0e62fb6a574a9ab4493c026906d16c-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cd0e62fb6a574a9ab4493c026906d16c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cd0e62fb6a574a9ab4493c026906d16c-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cd0e62fb6a574a9ab4493c026906d16c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cd0e62fb6a574a9ab4493c026906d16c-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cd0e62fb6a574a9ab4493c026906d16c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cd0e62fb6a574a9ab4493c026906d16c-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cd0e62fb6a574a9ab4493c026906d16c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cd0e62fb6a574a9ab4493c026906d16c-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cd0e62fb6a574a9ab4493c026906d16c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cd0e62fb6a574a9ab4493c026906d16c-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cd0e62fb6a574a9ab4493c026906d16c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cd0e62fb6a574a9ab4493c026906d16c-0-7\" stroke-width=\"2px\" d=\"M945,264.5 C945,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cd0e62fb6a574a9ab4493c026906d16c-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in.Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.\"\n",
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "    print(\">\", sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrj2LFljgYB4",
        "outputId": "95a8a4d1-aa28-4a2c-db49-a9e89d9c8c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit.\n",
            "> I fell in.\n",
            "> Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket.\n",
            "> The gorillas just went wild.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Named entity Recognition\n",
        "text = \"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAk60W5BgnIR",
        "outputId": "c6d23468-4097-42a7-f29f-356a18babb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steve Jobs\n",
            "Steve Wozniak\n",
            "Apple Computer\n",
            "January\n",
            "Cupertino\n",
            "California\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rm_rlAVgtmx",
        "outputId": "1911140c-a55c-4413-8d5e-35f94eaa111f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steve Jobs PERSON\n",
            "Steve Wozniak PERSON\n",
            "Apple Computer ORG\n",
            "January 3, 1977 DATE\n",
            "Cupertino GPE\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc, style=\"ent\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "s4d-kblJgw8-",
        "outputId": "44b48097-22a2-4ccb-c98a-748fcd1e0c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Steve Jobs\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Steve Wozniak\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " incorporated \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple Computer\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    January 3, 1977\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Cupertino\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    California\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaIoRV4tg7zb",
        "outputId": "892b1ebe-6b8d-445f-9609-38359b4a9284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy_wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtkIPx6shIIs",
        "outputId": "1cb948a4-a252-4fd6-faa1-c6f3aed143de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy_wordnet in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: nltk<3.6,>=3.3 in /usr/local/lib/python3.10/dist-packages (from spacy_wordnet) (3.5)\n",
            "Requirement already satisfied: spacy>=2 in /usr/local/lib/python3.10/dist-packages (from spacy_wordnet) (3.7.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<3.6,>=3.3->spacy_wordnet) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<3.6,>=3.3->spacy_wordnet) (1.4.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk<3.6,>=3.3->spacy_wordnet) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<3.6,>=3.3->spacy_wordnet) (4.66.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2->spacy_wordnet) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2->spacy_wordnet) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2->spacy_wordnet) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2->spacy_wordnet) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2->spacy_wordnet) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy_wordnet) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy_wordnet) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy_wordnet) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy_wordnet) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2->spacy_wordnet) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2->spacy_wordnet) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2->spacy_wordnet) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2->spacy_wordnet) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2->spacy_wordnet) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2->spacy_wordnet) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2->spacy_wordnet) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2->spacy_wordnet) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2->spacy_wordnet) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2->spacy_wordnet) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2->spacy_wordnet) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2->spacy_wordnet) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "print(\"before\", nlp.pipe_names)\n",
        "if \"spacy_wordnet\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"spacy_wordnet\", after=\"tagger\")\n",
        "print(\"after\", nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nCEI-l_g_-w",
        "outputId": "d20d3a05-2512-464b-981d-b2a146ca80e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before ['tok2vec', 'tagger', 'spacy_wordnet', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "after ['tok2vec', 'tagger', 'spacy_wordnet', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = nlp(\"withdraw\")[0]\n",
        "token._.wordnet.synsets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_x_lUZKh_65",
        "outputId": "a5509bcc-aa54-4060-d28b-83279250f1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('withdraw.v.01'),\n",
              " Synset('retire.v.02'),\n",
              " Synset('disengage.v.01'),\n",
              " Synset('recall.v.07'),\n",
              " Synset('swallow.v.05'),\n",
              " Synset('seclude.v.01'),\n",
              " Synset('adjourn.v.02'),\n",
              " Synset('bow_out.v.02'),\n",
              " Synset('withdraw.v.09'),\n",
              " Synset('retire.v.08'),\n",
              " Synset('retreat.v.04'),\n",
              " Synset('remove.v.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token._.wordnet.lemmas()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv-Dqr-jiFSw",
        "outputId": "cd83e236-1280-4c1a-dacc-f8ca251f94bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('withdraw.v.01.withdraw'),\n",
              " Lemma('withdraw.v.01.retreat'),\n",
              " Lemma('withdraw.v.01.pull_away'),\n",
              " Lemma('withdraw.v.01.draw_back'),\n",
              " Lemma('withdraw.v.01.recede'),\n",
              " Lemma('withdraw.v.01.pull_back'),\n",
              " Lemma('withdraw.v.01.retire'),\n",
              " Lemma('withdraw.v.01.move_back'),\n",
              " Lemma('retire.v.02.retire'),\n",
              " Lemma('retire.v.02.withdraw'),\n",
              " Lemma('disengage.v.01.disengage'),\n",
              " Lemma('disengage.v.01.withdraw'),\n",
              " Lemma('recall.v.07.recall'),\n",
              " Lemma('recall.v.07.call_in'),\n",
              " Lemma('recall.v.07.call_back'),\n",
              " Lemma('recall.v.07.withdraw'),\n",
              " Lemma('swallow.v.05.swallow'),\n",
              " Lemma('swallow.v.05.take_back'),\n",
              " Lemma('swallow.v.05.unsay'),\n",
              " Lemma('swallow.v.05.withdraw'),\n",
              " Lemma('seclude.v.01.seclude'),\n",
              " Lemma('seclude.v.01.sequester'),\n",
              " Lemma('seclude.v.01.sequestrate'),\n",
              " Lemma('seclude.v.01.withdraw'),\n",
              " Lemma('adjourn.v.02.adjourn'),\n",
              " Lemma('adjourn.v.02.withdraw'),\n",
              " Lemma('adjourn.v.02.retire'),\n",
              " Lemma('bow_out.v.02.bow_out'),\n",
              " Lemma('bow_out.v.02.withdraw'),\n",
              " Lemma('withdraw.v.09.withdraw'),\n",
              " Lemma('withdraw.v.09.draw'),\n",
              " Lemma('withdraw.v.09.take_out'),\n",
              " Lemma('withdraw.v.09.draw_off'),\n",
              " Lemma('retire.v.08.retire'),\n",
              " Lemma('retire.v.08.withdraw'),\n",
              " Lemma('retreat.v.04.retreat'),\n",
              " Lemma('retreat.v.04.pull_back'),\n",
              " Lemma('retreat.v.04.back_out'),\n",
              " Lemma('retreat.v.04.back_away'),\n",
              " Lemma('retreat.v.04.crawfish'),\n",
              " Lemma('retreat.v.04.crawfish_out'),\n",
              " Lemma('retreat.v.04.pull_in_one's_horns'),\n",
              " Lemma('retreat.v.04.withdraw'),\n",
              " Lemma('remove.v.01.remove'),\n",
              " Lemma('remove.v.01.take'),\n",
              " Lemma('remove.v.01.take_away'),\n",
              " Lemma('remove.v.01.withdraw')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token._.wordnet.wordnet_domains()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5_5eqrdiHfN",
        "outputId": "9e0c8a61-aa7b-4c0f-e28a-79ccfe68bc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['astronomy',\n",
              " 'school',\n",
              " 'telegraphy',\n",
              " 'industry',\n",
              " 'psychology',\n",
              " 'ethnology',\n",
              " 'ethnology',\n",
              " 'administration',\n",
              " 'school',\n",
              " 'finance',\n",
              " 'economy',\n",
              " 'exchange',\n",
              " 'banking',\n",
              " 'commerce',\n",
              " 'medicine',\n",
              " 'ethnology',\n",
              " 'university',\n",
              " 'school',\n",
              " 'buildings',\n",
              " 'factotum',\n",
              " 'agriculture',\n",
              " 'mechanics',\n",
              " 'gastronomy',\n",
              " 'meteorology',\n",
              " 'physics',\n",
              " 'basketball',\n",
              " 'anatomy',\n",
              " 'skiing',\n",
              " 'nautical',\n",
              " 'engineering',\n",
              " 'racing',\n",
              " 'home',\n",
              " 'drawing',\n",
              " 'dentistry',\n",
              " 'ethnology',\n",
              " 'mathematics',\n",
              " 'furniture',\n",
              " 'animal_husbandry',\n",
              " 'industry',\n",
              " 'economy',\n",
              " 'body_care',\n",
              " 'chemistry',\n",
              " 'medicine',\n",
              " 'surgery',\n",
              " 'vehicles',\n",
              " 'transport',\n",
              " 'atomic_physic',\n",
              " 'archaeology',\n",
              " 'hydraulics',\n",
              " 'oceanography',\n",
              " 'golf',\n",
              " 'sculpture',\n",
              " 'earth',\n",
              " 'applied_science',\n",
              " 'artisanship']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets extract lemmas for specific domain (lets replace word with set of synonym-lemmas given the domain)\n",
        "domains = [\"finance\", \"banking\", \"surgery\"]\n",
        "sentence = nlp(\"I want to withdraw 5,000 dollars.\")\n",
        "enriched_text = []\n",
        "for token in sentence:\n",
        "  # get the synonym sets for each token in given domain\n",
        "  synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
        "  if synsets:\n",
        "    lemmas_for_synset = []\n",
        "    # loop over each item in the synset and extend its lemmas (as done in previous class)\n",
        "    for s in synsets:\n",
        "      lemmas_for_synset.extend(s.lemma_names())\n",
        "    enriched_text.append(f\"({'|'.join(set(lemmas_for_synset))})\")\n",
        "  else:\n",
        "    enriched_text.append(token.text)\n",
        "\" \".join(enriched_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mRrw_xMgp6Vh",
        "outputId": "ca42eccc-a2dd-4e90-c265-bfee74728122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I (need|want|require) to (take_out|withdraw|remove|take_away|take|draw_off|draw) 5,000 (dollar) .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}